{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'01_palm': 0,\n",
       " '02_l': 1,\n",
       " '03_fist': 2,\n",
       " '04_fist_moved': 3,\n",
       " '05_thumb': 4,\n",
       " '06_index': 5,\n",
       " '07_ok': 6,\n",
       " '08_palm_moved': 7,\n",
       " '09_c': 8,\n",
       " '10_down': 9}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # We'll be storing our data as numpy arrays\n",
    "import os # For handling directories\n",
    "from PIL import Image # For handling the images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg # Plotting\n",
    "\n",
    "lookup = dict()\n",
    "reverselookup = dict()\n",
    "count = 0\n",
    "for j in os.listdir('Train_Final/leapGestRecog/00/'):\n",
    "    if not j.startswith('.'): # If running this code locally, this is to \n",
    "                              # ensure you aren't reading in hidden folders\n",
    "        lookup[j] = count\n",
    "        reverselookup[count] = j\n",
    "        count = count + 1\n",
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 10 classes.\n",
      "Found 4000 images belonging to 10 classes.\n",
      "Found 2000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------------------------------\n",
    "# To prepare dataset including train_set, validation_set, and test_set\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator can create an object used to generate batches of image data\n",
    "image_datagen = ImageDataGenerator(rescale = 1./255, dtype='float32', validation_split=0.2)\n",
    "\n",
    "# The ImageDataGenerator class has the method flow_from_directory() \n",
    "#       wihch can read the images from folders containing images.\n",
    "train_set = image_datagen.flow_from_directory(  'Train_Final/leapGestRecog/',\n",
    "                                                 #target_size = (640, 240),\n",
    "                                                 target_size = (200, 200),\n",
    "                                                 batch_size  = 64,\n",
    "                                                 class_mode  = 'categorical',\n",
    "                                                 subset='training'\n",
    "                                                 )\n",
    "\n",
    "validation_set = image_datagen.flow_from_directory(  'Train_Final/leapGestRecog/',\n",
    "                                                     #target_size = (640, 240),\n",
    "                                                     target_size = (200, 200),\n",
    "                                                     batch_size  = 64,\n",
    "                                                     class_mode  = 'categorical',\n",
    "                                                     subset='validation'\n",
    "                                                     )\n",
    "\n",
    "test_set = image_datagen.flow_from_directory(   'Test/',\n",
    "                                                #target_size = (640, 240),\n",
    "                                                target_size = (200, 200),\n",
    "                                                batch_size  = 32,\n",
    "                                                class_mode  = 'categorical',\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.models import Sequential,load_model\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top classification layer\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "\n",
    "# Freeze the pre-trained layers in VGG16\n",
    "# for layer in vgg16.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Convert single-channel input to three channels\n",
    "model = Sequential()\n",
    "# model.add(tf.keras.layers.Lambda(lambda x: tf.concat([x, x, x], axis=-1), input_shape=(120, 320, 1)))\n",
    "\n",
    "# Add the VGG16 base model\n",
    "model.add(vgg16)\n",
    "\n",
    "# Add custom classification layers on top of VGG16\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "         optimizer = 'rmsprop',\n",
    "         metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(train_set, epochs = 3, batch_size=64, validation_data=validation_set)\n",
    "\n",
    "print(\"Train accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Val accuracy:\", history.history['val_accuracy'][-1])\n",
    "\n",
    "print(\"Train loss:\", history.history['loss'][-1])\n",
    "print(\"Val loss:\", history.history['val_loss'][-1])\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_set)\n",
    "\n",
    "print('Test accuracy: {:2.2f}%'.format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from skimage.segmentation import slic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top classification layer\n",
    "# vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(120, 320, 1))\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "\n",
    "# Freeze the pre-trained layers in VGG16\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the complete model with VGG16 base\n",
    "model = Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_set, epochs=5, batch_size=10, validation_data=validation_set)\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = lime.lime_image.LimeImageExplainer()\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_fn(images):\n",
    "    preprocessed_images = preprocess_input(images)\n",
    "    return model.predict(preprocessed_images)\n",
    "\n",
    "# Keep track of seen classes and corresponding Lime explanations\n",
    "seen_classes = set()\n",
    "lime_explanations = []\n",
    "\n",
    "# Iterate over the test set and explain a single sample from each class using LIME\n",
    "for i in range(len(test_set)):\n",
    "    sample_image = test_set[i][0]\n",
    "    sample_label = np.argmax(test_set[i][1])\n",
    "    \n",
    "    # Check if the current sample belongs to a new class\n",
    "    if sample_label not in seen_classes:\n",
    "        # Convert the single-channel grayscale image to three-channel RGB image\n",
    "        sample_image_rgb = np.repeat(sample_image, 3, axis=-1)\n",
    "        \n",
    "        # Reshape the sample_image to match the expected input shape of the model\n",
    "        sample_image_rgb = np.expand_dims(sample_image_rgb, axis=0)\n",
    "        \n",
    "        # Apply superpixel segmentation using SLIC algorithm\n",
    "        segments = slic(sample_image_rgb[0], n_segments=100, compactness=10, sigma=1)\n",
    "        \n",
    "        # Explain the image using LIME\n",
    "        explanation = explainer.explain_instance(sample_image_rgb[0], predict_fn, top_labels=1, num_samples=1000)\n",
    "        \n",
    "        # Add the explanation to the lime_explanations list\n",
    "        lime_explanations.append((sample_image_rgb[0], explanation))\n",
    "        \n",
    "        # Add the class to the seen_classes set\n",
    "        seen_classes.add(sample_label)\n",
    "        \n",
    "        if len(seen_classes) == 10:\n",
    "            break  # Exit the loop if all 10 classes are seen\n",
    "\n",
    "# Display the Lime explanations\n",
    "fig, axes = plt.subplots(10, 2, figsize=(8, 30))\n",
    "\n",
    "for i, (image, explanation) in enumerate(lime_explanations):\n",
    "    ax1 = axes[i, 0]\n",
    "    ax2 = axes[i, 1]\n",
    "    \n",
    "    # Show the original image\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Label: {}'.format(list(seen_classes)[i]))\n",
    "    \n",
    "    # Show the Lime explanation\n",
    "    lime_image, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "    ax2.imshow(lime_image)\n",
    "    ax2.imshow(mask, cmap='jet', alpha=0.7)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('LIME Explanation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from skimage.segmentation import slic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top classification layer\n",
    "# vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(120, 320, 1))\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "\n",
    "# Freeze the pre-trained layers in VGG16\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the complete model with VGG16 base\n",
    "model = Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_set, epochs=3, batch_size=10, validation_data=validation_set)\n",
    "\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = lime.lime_image.LimeImageExplainer()\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_fn(images):\n",
    "    return model.predict(images)\n",
    "\n",
    "# Keep track of seen classes and corresponding Lime explanations\n",
    "seen_classes = set()\n",
    "lime_explanations = []\n",
    "\n",
    "# Iterate over the test set and explain 2 samples from each class using LIME\n",
    "for i in range(len(test_set)):\n",
    "    sample_images = test_set[i][0]\n",
    "    sample_labels = test_set[i][1]\n",
    "    \n",
    "    for j in range(len(sample_images)):\n",
    "        sample_image = sample_images[j]\n",
    "        sample_label = np.argmax(sample_labels[j])\n",
    "\n",
    "        # Check if the current sample belongs to a new class or still requires more explanations\n",
    "        if (len(lime_explanations) < 2) or (sample_label not in seen_classes and len(seen_classes) < 10):\n",
    "            # Reshape the sample_image to match the expected input shape of the model\n",
    "            sample_image = np.expand_dims(sample_image, axis=0)\n",
    "            \n",
    "            # Apply superpixel segmentation using SLIC algorithm\n",
    "            segments = slic(sample_image[0], n_segments=100, compactness=10, sigma=1)\n",
    "            \n",
    "            # Explain the image using LIME\n",
    "            explanation = explainer.explain_instance(sample_image[0], predict_fn, top_labels=1, num_samples=1000, segmentation_fn=lambda x: segments)\n",
    "            \n",
    "            # Add the explanation to the lime_explanations list\n",
    "            lime_explanations.append((sample_image[0], explanation))\n",
    "            \n",
    "            # Add the class to the seen_classes set\n",
    "            seen_classes.add(sample_label)\n",
    "            \n",
    "        if len(lime_explanations) >= 20:\n",
    "            break  # Exit the loop if all 20 samples (2 samples from each class) are obtained\n",
    "            \n",
    "    if len(lime_explanations) >= 20:\n",
    "        break  # Exit the outer loop if all 20 samples (2 samples from each class) are obtained\n",
    "\n",
    "# Display the Lime explanations\n",
    "fig, axes = plt.subplots(10, 4, figsize=(12, 20))\n",
    "\n",
    "for i, (image, explanation) in enumerate(lime_explanations):\n",
    "    ax1 = axes[i // 2, i % 2 * 2]\n",
    "    ax2 = axes[i // 2, i % 2 * 2 + 1]\n",
    "    \n",
    "    # Show the original image\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Label: {}'.format(list(seen_classes)[i]))\n",
    "    \n",
    "    # Show the Lime explanation\n",
    "    lime_image, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "    ax2.imshow(lime_image)\n",
    "    ax2.imshow(mask, cmap='jet', alpha=0.7)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('LIME Explanation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 5s 78ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only 2D color images are supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14536\\3581454117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Generate an explanation using Lime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mexplanation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhide_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# Show the explanation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SHAKIB\\anaconda3\\envs\\new_gpu\\lib\\site-packages\\lime\\lime_image.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0msegments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmentation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mfudged_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SHAKIB\\anaconda3\\envs\\new_gpu\\lib\\site-packages\\lime\\lime_image.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[0;32m    180\u001b[0m                                                     random_seed=random_seed)\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[0msegments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmentation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SHAKIB\\anaconda3\\envs\\new_gpu\\lib\\site-packages\\lime\\wrappers\\scikit_image.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\SHAKIB\\anaconda3\\envs\\new_gpu\\lib\\site-packages\\skimage\\segmentation\\_quickshift.py\u001b[0m in \u001b[0;36mquickshift\u001b[1;34m(image, ratio, kernel_size, max_dist, return_tree, sigma, convert2lab, random_seed, channel_axis)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"only 2D color images are supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# move channels to last position as expected by the Cython code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only 2D color images are supported"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from skimage.segmentation import slic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top classification layer\n",
    "# vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(120, 320, 1))\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "\n",
    "# Freeze the pre-trained layers in VGG16\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the complete model with VGG16 base\n",
    "model = Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#history = model.fit(train_set, epochs=3, batch_size=10, validation_data=validation_set)\n",
    "\n",
    "# Initialize LimeImageExplainer\n",
    "explainer = lime.lime_image.LimeImageExplainer()\n",
    "\n",
    "# Choose a random image from your validation set\n",
    "image = validation_set[0][0]  # Access the first element of the tuple, assuming it contains the image\n",
    "\n",
    "# Reshape the image to have a single color channel\n",
    "image = np.squeeze(image)  # Remove any single-dimensional entries from the shape\n",
    "image = np.expand_dims(image, axis=2)  # Add a single color channel\n",
    "\n",
    "# Define a function to predict using your model\n",
    "predict_fn = model.predict(test_set)  # Use 1 channel instead of 3\n",
    "\n",
    "# Generate an explanation using Lime\n",
    "explanation = explainer.explain_instance(image, predict_fn, top_labels=1, hide_color=0, num_samples=1000)\n",
    "\n",
    "# Show the explanation\n",
    "explanation.show_in_notebook(text=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
